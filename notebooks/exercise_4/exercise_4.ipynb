{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e52763a-ba87-4a1e-8f68-51fcac37742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice to have and only here as a reference until moved to its instructional home :)\n",
    "#export CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn; print(nvidia.cudnn.__file__)\"))\n",
    "#export SITE_PACKAGES_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")\n",
    "#export LD_LIBRARY_PATH=$CUDNN_PATH/lib:$SITE_PACKAGES_PATH/tensorrt_libs/:$LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39b271c-785e-45aa-9f2d-10946bb9a2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/flaniganp/mambaforge/envs/torch_exercise_4:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                 conda_forge    conda-forge\n",
      "_openmp_mutex             4.5                       2_gnu    conda-forge\n",
      "anyio                     4.3.0                    pypi_0    pypi\n",
      "argon2-cffi               23.1.0                   pypi_0    pypi\n",
      "argon2-cffi-bindings      21.2.0                   pypi_0    pypi\n",
      "arrow                     1.3.0                    pypi_0    pypi\n",
      "asttokens                 2.4.1                    pypi_0    pypi\n",
      "async-lru                 2.0.4                    pypi_0    pypi\n",
      "attrs                     23.2.0                   pypi_0    pypi\n",
      "babel                     2.14.0                   pypi_0    pypi\n",
      "beautifulsoup4            4.12.3                   pypi_0    pypi\n",
      "bleach                    6.1.0                    pypi_0    pypi\n",
      "bzip2                     1.0.8                hd590300_5    conda-forge\n",
      "ca-certificates           2024.2.2             hbcca054_0    conda-forge\n",
      "certifi                   2024.2.2                 pypi_0    pypi\n",
      "cffi                      1.16.0                   pypi_0    pypi\n",
      "charset-normalizer        3.3.2                    pypi_0    pypi\n",
      "comm                      0.2.1                    pypi_0    pypi\n",
      "debugpy                   1.8.1                    pypi_0    pypi\n",
      "decorator                 5.1.1                    pypi_0    pypi\n",
      "defusedxml                0.7.1                    pypi_0    pypi\n",
      "exceptiongroup            1.2.0                    pypi_0    pypi\n",
      "executing                 2.0.1                    pypi_0    pypi\n",
      "fastjsonschema            2.19.1                   pypi_0    pypi\n",
      "filelock                  3.13.1                   pypi_0    pypi\n",
      "fqdn                      1.5.1                    pypi_0    pypi\n",
      "fsspec                    2024.2.0                 pypi_0    pypi\n",
      "h11                       0.14.0                   pypi_0    pypi\n",
      "httpcore                  1.0.4                    pypi_0    pypi\n",
      "httpx                     0.27.0                   pypi_0    pypi\n",
      "idna                      3.6                      pypi_0    pypi\n",
      "ipykernel                 6.29.3                   pypi_0    pypi\n",
      "ipython                   8.22.2                   pypi_0    pypi\n",
      "isoduration               20.11.0                  pypi_0    pypi\n",
      "jedi                      0.19.1                   pypi_0    pypi\n",
      "jinja2                    3.1.3                    pypi_0    pypi\n",
      "joblib                    1.3.2                    pypi_0    pypi\n",
      "json5                     0.9.20                   pypi_0    pypi\n",
      "jsonpointer               2.4                      pypi_0    pypi\n",
      "jsonschema                4.21.1                   pypi_0    pypi\n",
      "jsonschema-specifications 2023.12.1                pypi_0    pypi\n",
      "jupyter-client            8.6.0                    pypi_0    pypi\n",
      "jupyter-core              5.7.1                    pypi_0    pypi\n",
      "jupyter-events            0.9.0                    pypi_0    pypi\n",
      "jupyter-lsp               2.2.4                    pypi_0    pypi\n",
      "jupyter-server            2.13.0                   pypi_0    pypi\n",
      "jupyter-server-terminals  0.5.2                    pypi_0    pypi\n",
      "jupyterlab                4.1.3                    pypi_0    pypi\n",
      "jupyterlab-pygments       0.3.0                    pypi_0    pypi\n",
      "jupyterlab-server         2.25.3                   pypi_0    pypi\n",
      "ld_impl_linux-64          2.40                 h41732ed_0    conda-forge\n",
      "libffi                    3.4.2                h7f98852_5    conda-forge\n",
      "libgcc-ng                 13.2.0               h807b86a_5    conda-forge\n",
      "libgomp                   13.2.0               h807b86a_5    conda-forge\n",
      "libnsl                    2.0.1                hd590300_0    conda-forge\n",
      "libsqlite                 3.45.1               h2797004_0    conda-forge\n",
      "libuuid                   2.38.1               h0b41bf4_0    conda-forge\n",
      "libxcrypt                 4.4.36               hd590300_1    conda-forge\n",
      "libzlib                   1.2.13               hd590300_5    conda-forge\n",
      "matplotlib-inline         0.1.6                    pypi_0    pypi\n",
      "mistune                   3.0.2                    pypi_0    pypi\n",
      "mpmath                    1.3.0                    pypi_0    pypi\n",
      "nbclient                  0.9.0                    pypi_0    pypi\n",
      "nbconvert                 7.16.2                   pypi_0    pypi\n",
      "nbformat                  5.9.2                    pypi_0    pypi\n",
      "ncurses                   6.4                  h59595ed_2    conda-forge\n",
      "nest-asyncio              1.6.0                    pypi_0    pypi\n",
      "networkx                  3.2.1                    pypi_0    pypi\n",
      "notebook                  7.1.1                    pypi_0    pypi\n",
      "notebook-shim             0.2.4                    pypi_0    pypi\n",
      "numpy                     1.26.4                   pypi_0    pypi\n",
      "nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\n",
      "nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\n",
      "nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\n",
      "nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\n",
      "nvidia-cudnn-cu12         8.9.2.26                 pypi_0    pypi\n",
      "nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\n",
      "nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\n",
      "nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\n",
      "nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\n",
      "nvidia-nccl-cu12          2.19.3                   pypi_0    pypi\n",
      "nvidia-nvjitlink-cu12     12.3.101                 pypi_0    pypi\n",
      "nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\n",
      "openssl                   3.2.1                hd590300_0    conda-forge\n",
      "overrides                 7.7.0                    pypi_0    pypi\n",
      "pandocfilters             1.5.1                    pypi_0    pypi\n",
      "parso                     0.8.3                    pypi_0    pypi\n",
      "pexpect                   4.9.0                    pypi_0    pypi\n",
      "pip                       24.0               pyhd8ed1ab_0    conda-forge\n",
      "platformdirs              4.2.0                    pypi_0    pypi\n",
      "prometheus-client         0.20.0                   pypi_0    pypi\n",
      "prompt-toolkit            3.0.43                   pypi_0    pypi\n",
      "psutil                    5.9.8                    pypi_0    pypi\n",
      "ptyprocess                0.7.0                    pypi_0    pypi\n",
      "pure-eval                 0.2.2                    pypi_0    pypi\n",
      "pycparser                 2.21                     pypi_0    pypi\n",
      "pygments                  2.17.2                   pypi_0    pypi\n",
      "python                    3.10.13         hd12c33a_1_cpython    conda-forge\n",
      "python-dateutil           2.9.0.post0              pypi_0    pypi\n",
      "python-json-logger        2.0.7                    pypi_0    pypi\n",
      "pyyaml                    6.0.1                    pypi_0    pypi\n",
      "pyzmq                     25.1.2                   pypi_0    pypi\n",
      "readline                  8.2                  h8228510_1    conda-forge\n",
      "referencing               0.33.0                   pypi_0    pypi\n",
      "requests                  2.31.0                   pypi_0    pypi\n",
      "rfc3339-validator         0.1.4                    pypi_0    pypi\n",
      "rfc3986-validator         0.1.1                    pypi_0    pypi\n",
      "rpds-py                   0.18.0                   pypi_0    pypi\n",
      "scikit-learn              1.4.1.post1              pypi_0    pypi\n",
      "scipy                     1.12.0                   pypi_0    pypi\n",
      "send2trash                1.8.2                    pypi_0    pypi\n",
      "setuptools                69.1.1             pyhd8ed1ab_0    conda-forge\n",
      "six                       1.16.0                   pypi_0    pypi\n",
      "sniffio                   1.3.1                    pypi_0    pypi\n",
      "soupsieve                 2.5                      pypi_0    pypi\n",
      "stack-data                0.6.3                    pypi_0    pypi\n",
      "sympy                     1.12                     pypi_0    pypi\n",
      "terminado                 0.18.0                   pypi_0    pypi\n",
      "threadpoolctl             3.3.0                    pypi_0    pypi\n",
      "tinycss2                  1.2.1                    pypi_0    pypi\n",
      "tk                        8.6.13          noxft_h4845f30_101    conda-forge\n",
      "tomli                     2.0.1                    pypi_0    pypi\n",
      "torch                     2.2.1                    pypi_0    pypi\n",
      "torchdata                 0.7.1                    pypi_0    pypi\n",
      "torchtext                 0.17.1                   pypi_0    pypi\n",
      "tornado                   6.4                      pypi_0    pypi\n",
      "tqdm                      4.66.2                   pypi_0    pypi\n",
      "traitlets                 5.14.1                   pypi_0    pypi\n",
      "triton                    2.2.0                    pypi_0    pypi\n",
      "types-python-dateutil     2.8.19.20240106          pypi_0    pypi\n",
      "typing-extensions         4.10.0                   pypi_0    pypi\n",
      "tzdata                    2024a                h0c530f3_0    conda-forge\n",
      "uri-template              1.3.0                    pypi_0    pypi\n",
      "urllib3                   2.2.1                    pypi_0    pypi\n",
      "wcwidth                   0.2.13                   pypi_0    pypi\n",
      "webcolors                 1.13                     pypi_0    pypi\n",
      "webencodings              0.5.1                    pypi_0    pypi\n",
      "websocket-client          1.7.0                    pypi_0    pypi\n",
      "wheel                     0.42.0             pyhd8ed1ab_0    conda-forge\n",
      "xz                        5.2.6                h166bdaf_0    conda-forge\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a50bfb9-e343-41e6-9bd4-72e56749e5b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m search\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# This line imports the 'check_output' function from the 'subprocess' module in Python. The 'subprocess' module\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes. This\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# module is intended to replace older modules and functions like os.system and os.spawn*.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# and test subsets. This is crucial for evaluating the performance of machine learning models by training on one subset\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# of the data and testing on another.\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Example Usage:\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Suppose you want to capture the output of the 'ls' command in a Unix/Linux system. You can use 'check_output' like\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# this:\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# output = check_output(['ls', '-l'])\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_output\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# OrderedDict from collections: Provides a dictionary subclass that preserves the order in which keys were first added.\n",
    "# This is useful for maintaining an ordered set of elements, especially when the order of elements matters for\n",
    "# operations.\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Counter from collections: A specialized dictionary subclass designed for counting hashable objects.\n",
    "# It is a collection where elements are stored as dictionary keys and their counts are stored as dictionary values.\n",
    "from collections import Counter\n",
    "\n",
    "# load from json: A function to parse a JSON formatted string or file into a Python dictionary or list.\n",
    "# It is commonly used for reading data from JSON files or parsing JSON responses from APIs.\n",
    "from json import load as json_load\n",
    "\n",
    "# The os module in Python provides a way of using operating system dependent functionality. It allows you to interface\n",
    "# with the underlying operating system that Python is running on â€“ be it Windows, Mac or Linux. You can use the os module\n",
    "# to handle file and directory paths, create folders, list contents of a directory, manage environment variables, execute\n",
    "# shell commands, and more.\n",
    "import os\n",
    "\n",
    "# Regular Expressions\n",
    "# 1. search: This function is used to perform a search for a pattern in a string and returns a match object if the\n",
    "# pattern is found, otherwise None. It's particularly useful for string pattern matching and extracting specific\n",
    "# segments from text.\n",
    "from re import search\n",
    "\n",
    "# This line imports the 'check_output' function from the 'subprocess' module in Python. The 'subprocess' module\n",
    "# allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes. This\n",
    "# module is intended to replace older modules and functions like os.system and os.spawn*.\n",
    "# Key aspects of 'check_output':\n",
    "# 1. **Process Execution**: The 'check_output' function is used to run a command in the subprocess/external process and\n",
    "#    capture its output. This is especially useful for running system commands and capturing their output directly\n",
    "#    within a Python script.\n",
    "# 2. **Return Output**: It returns the output of the command, making it available to the Python environment. If the\n",
    "#    called command results in an error (non-zero exit status), it raises a CalledProcessError.\n",
    "# 3. **Use Cases**: Common use cases include executing a shell command, reading the output of a command, automating\n",
    "#    scripts that interact with the command line, and integrating external tools into a Python workflow.\n",
    "\n",
    "# train_test_split from sklearn.model_selection: A function used to split data arrays or matrices into random train\n",
    "# and test subsets. This is crucial for evaluating the performance of machine learning models by training on one subset\n",
    "# of the data and testing on another.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example Usage:\n",
    "# Suppose you want to capture the output of the 'ls' command in a Unix/Linux system. You can use 'check_output' like\n",
    "# this:\n",
    "# output = check_output(['ls', '-l'])\n",
    "from subprocess import check_output\n",
    "\n",
    "# Importing the PyTorch library, known as `torch`, a powerful and widely used open-source machine learning framework.\n",
    "# PyTorch provides tools and libraries for designing, training, and deploying deep learning models with ease. It's\n",
    "# particularly known for its flexibility, user-friendly interface, and dynamic computational graph that allows for\n",
    "# adaptive and efficient deep learning development. By importing `torch`, you gain access to a vast range of\n",
    "# functionalities for handling multi-dimensional arrays (tensors), performing complex mathematical operations,\n",
    "# and utilizing GPUs for accelerated computing. This makes it an indispensable tool for both researchers and\n",
    "# developers in the field of artificial intelligence.\n",
    "import torch\n",
    "\n",
    "# nn from torch: Provides the building blocks for creating neural networks, including layers, activation functions,\n",
    "# and loss functions. It's a foundational module for defining and assembling neural network architectures.\n",
    "import torch.nn as nn\n",
    "\n",
    "# nn.functional from torch: Contains functions like activation functions, loss functions, and convolution operations.\n",
    "# These functions are stateless, providing a functional interface to operations that can be applied to tensors.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# optim from torch: Implements various optimization algorithms for training neural networks, including SGD, Adam,\n",
    "# and RMSprop. These algorithms are used to update the weights of the network during training.\n",
    "import torch.optim as optim\n",
    "\n",
    "# Imports the quantize_dynamic function from PyTorch's quantization submodule. This function is used for dynamically \n",
    "# quantizing the weights of specified layers (e.g., Linear, LSTM) in a trained PyTorch model to int8 format, reducing\n",
    "# model size and potentially increasing inference speed without needing input data for calibration.\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "# DataLoader and Dataset from torch.utils.data: DataLoader provides an iterable over a dataset, with support for\n",
    "# batching, sampling, shuffling, and multiprocess data loading. Dataset is an abstract class for representing\n",
    "# a dataset, with custom implementations required to define how data is loaded and processed.\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# get_tokenizer from torchtext.data.utils: A utility function to obtain a tokenizer. Tokenizers are used to convert\n",
    "# text into a sequence of tokens or words, which is a common preprocessing step for text data in natural language\n",
    "# processing.\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# vocab from torchtext.vocab: A class for building a vocabulary from a set of tokens. This vocabulary can then map\n",
    "# tokens to indices, which is a common requirement for converting text data into a numerical form that can be processed\n",
    "# by models.\n",
    "from torchtext.vocab import vocab as torch_text_vocab\n",
    "\n",
    "# urlretrieve from urllib.request: A utility function to download a file from a given URL. This is useful for\n",
    "# downloading data sets, pre-trained models, or other resources required by a project from the internet.\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e062b8-f53e-44b0-b240-4483b58e02b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function `print_gpu_info` is designed to display detailed information about the available GPUs on the system.\n",
    "# It utilizes TensorFlow's `device_lib.list_local_devices()` method to enumerate all computing devices recognized by\n",
    "# TensorFlow. For each device identified as a GPU, the function extracts and prints relevant details including the GPU's\n",
    "# ID, name, memory limit (converted to megabytes), and compute capability. The extraction of GPU information involves\n",
    "# parsing the device's description string using regular expressions to find specific pieces of information. This\n",
    "# function can be particularly useful for debugging or for setting up configurations in environments with multiple GPUs,\n",
    "# ensuring that TensorFlow is utilizing the GPUs as expected.\n",
    "\n",
    "def print_gpu_info():\n",
    "    # Undocumented Method\n",
    "    # https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow\n",
    "    # Get the list of all devices\n",
    "    devices = device_lib.list_local_devices()\n",
    "\n",
    "    for device in devices:\n",
    "        if device.device_type == 'GPU':\n",
    "            # Extract the physical device description\n",
    "            desc = device.physical_device_desc\n",
    "\n",
    "            # Use regular expressions to extract the required information\n",
    "            gpu_id_match = search(r'device: (\\d+)', desc)\n",
    "            name_match = search(r'name: (.*?),', desc)\n",
    "            compute_capability_match = search(r'compute capability: (\\d+\\.\\d+)', desc)\n",
    "\n",
    "            if gpu_id_match and name_match and compute_capability_match:\n",
    "                gpu_id = gpu_id_match.group(1)\n",
    "                gpu_name = name_match.group(1)\n",
    "                compute_capability = compute_capability_match.group(1)\n",
    "\n",
    "                # Convert memory limit from bytes to gigabytes and round it\n",
    "                memory_limit_gb = round(device.memory_limit / (1024 ** 2))\n",
    "\n",
    "                print(\n",
    "                    f\"\\tGPU ID {gpu_id} --> {gpu_name} --> \"\n",
    "                    f\"Memory Limit {memory_limit_gb} MB --> \"\n",
    "                    f\"Compute Capability {compute_capability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1824c4-3d87-4aff-8260-daa0d9e05379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA Driver: 545.23.08\n",
      "Maximum Supported CUDA Version: 12.3     \n"
     ]
    }
   ],
   "source": [
    "# NVIDIA Driver\n",
    "try:\n",
    "    # Execute the nvidia-smi command and decode the output\n",
    "    nvidia_smi_output = check_output(\"nvidia-smi\", shell=True).decode()\n",
    "\n",
    "    # Split the output into lines\n",
    "    lines = nvidia_smi_output.split('\\n')\n",
    "\n",
    "    # Find the line containing the driver version\n",
    "    driver_line = next((line for line in lines if \"Driver Version\" in line), None)\n",
    "\n",
    "    # Extract the driver version number\n",
    "    if driver_line:\n",
    "        driver_version = driver_line.split('Driver Version: ')[1].split()[0]\n",
    "        print(\"NVIDIA Driver:\", driver_version)\n",
    "\n",
    "        # Extract the maximum supported CUDA version\n",
    "        cuda_version = driver_line.split('CUDA Version: ')[1].strip().replace(\"|\", \"\")\n",
    "        print(\"Maximum Supported CUDA Version:\", cuda_version)\n",
    "    else:\n",
    "        print(\"NVIDIA Driver Version or CUDA Version not found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error fetching NVIDIA Driver Version or CUDA Version:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ad3553-9d61-44cf-9a4d-7d0a19f502d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Versions:\n",
      "CUDA Version 11.8.89\n"
     ]
    }
   ],
   "source": [
    "print(\"Software Versions:\")\n",
    "\n",
    "# CUDA\n",
    "try:\n",
    "    # Execute the 'nvcc --version' command and decode the output\n",
    "    nvcc_output = check_output(\"nvcc --version\", shell=True).decode()\n",
    "\n",
    "    # Use regular expression to find the version number\n",
    "    match = search(r\"V(\\d+\\.\\d+\\.\\d+)\", nvcc_output)\n",
    "    if match:\n",
    "        cuda_version = match.group(1)\n",
    "        print(\"CUDA Version\", cuda_version)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        print(\"CUDA Version not found\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "except CalledProcessError as e:\n",
    "    print(\"Error executing nvcc --version:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8679440a-bfd4-4c93-abd1-6533c26cbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class named SarcasmDataset, which is a subclass of PyTorch's Dataset class.\n",
    "# This custom dataset class is used to handle sarcasm data (sentences and labels).\n",
    "\n",
    "# In Python, the use of double underscores (also known as \"dunder\" or \"magic\" methods) before and after the names of\n",
    "# certain methods, serves a specific purpose. These methods are part of Python's protocol for built-in behaviors and\n",
    "# are not meant to be called directly by the user, but rather by Python itself to perform certain operations.\n",
    "class SarcasmDataset(Dataset):\n",
    "\n",
    "    # The constructor method initializes the dataset object with sentences and their corresponding labels.\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.sentences = sentences  # Store the provided sentences in the instance variable.\n",
    "        self.labels = labels  # Store the provided labels in the instance variable.\n",
    "\n",
    "    # The __len__ method returns the number of items in the dataset.\n",
    "    # It's a required method for the Dataset class, allowing PyTorch to know the dataset size.\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)  # Return the total number of sentences.\n",
    "\n",
    "    # The __getitem__ method retrieves a single data point from the dataset.\n",
    "    # It's a required method for the Dataset class, enabling indexing and iteration over the dataset.\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the sentence and its corresponding label at the specified index.\n",
    "        # This allows for direct access to a data sample and its label using dataset[index].\n",
    "        return self.sentences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eee99a1-e4d3-4759-bd87-028d99da22cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function named create_vocab that takes a list of sentences and a maximum vocabulary size as input.\n",
    "def create_vocab(sentences, max_size):\n",
    "    # Get a tokenizer function for basic English from torchtext's utility functions. This tokenizer splits sentences\n",
    "    # into tokens (words).\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "    # Initialize a Counter object from the collections module to count occurrences of each token.\n",
    "    counter = Counter()\n",
    "\n",
    "    # Iterate over each sentence in the provided list of sentences.\n",
    "    for sentence in sentences:\n",
    "        # Update the counter with tokens from the current sentence, incrementing counts for each token found by the\n",
    "        # tokenizer.\n",
    "        counter.update(tokenizer(sentence))\n",
    "\n",
    "    # Retrieve the most common tokens up to the limit of max_size minus 2, to leave space for special tokens.\n",
    "    # This ensures the final vocabulary size doesn't exceed max_size.\n",
    "    most_common_tokens = counter.most_common(max_size - 2)  # Adjust for special tokens\n",
    "\n",
    "    # Create an OrderedDict and initialize it with two special tokens: '<pad>' for padding and '<unk>' for unknown\n",
    "    # tokens, assigning them indices 0 and 1, respectively.\n",
    "    extended_vocab = OrderedDict([('<pad>', 0), ('<unk>', 1)])\n",
    "\n",
    "    # Update the OrderedDict with the most common tokens, which will be assigned subsequent indices starting from 2.\n",
    "    extended_vocab.update(most_common_tokens)\n",
    "\n",
    "    # Create a Vocab object from the extended vocabulary containing both the special tokens and the most common tokens.\n",
    "    # The Vocab object provides a mapping from tokens to indices and is used for numericalizing text data.\n",
    "    return torch_text_vocab(extended_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7727c24c-1401-46d5-b15e-d512f67018f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function encode_sentences, which takes a list of sentences, a vocabulary, and a tokenizer as input\n",
    "# parameters.\n",
    "def encode_sentences(sentences, vocab, tokenizer):\n",
    "    # Initialize an empty list to hold the encoded versions of each sentence.\n",
    "    encoded_sentences = []\n",
    "\n",
    "    # Define a special token '<unk>' that represents any words not found in the vocabulary.\n",
    "    unk_token = '<unk>'\n",
    "\n",
    "    # Retrieve the index assigned to the unknown token '<unk>' in the provided vocabulary.\n",
    "    # This index will be used for words that are not present in the vocabulary.\n",
    "    unk_idx = vocab[unk_token]\n",
    "\n",
    "    # Iterate over each sentence in the list of sentences provided to the function.\n",
    "    for sentence in sentences:\n",
    "        # Use the provided tokenizer function to split the sentence into individual tokens (words).\n",
    "        tokenized_sentence = tokenizer(sentence)\n",
    "\n",
    "        # For each token in the tokenized sentence, check if the token exists in the vocabulary.\n",
    "        # If the token exists, use its corresponding index from the vocabulary. If not, use the index for '<unk>'.\n",
    "        # This results in a list of indices that represent the encoded sentence.\n",
    "        encoded_sentence = [vocab[token] if token in vocab else unk_idx for token in tokenized_sentence]\n",
    "\n",
    "        # Add the encoded sentence (a list of indices) to the list of encoded sentences.\n",
    "        encoded_sentences.append(encoded_sentence)\n",
    "\n",
    "    # After processing all sentences, return the list of encoded sentences.\n",
    "    return encoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77f80c43-5428-437b-9839-9efaddb032a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function pad_sequences, which takes a list of numerical sequences and a maximum sequence length as input\n",
    "# parameters.\n",
    "def pad_sequences(sequences, max_length):\n",
    "    # Create a tensor of zeros with the shape (number of sequences, max_length). This tensor will hold the padded\n",
    "    # sequences.\n",
    "    # The dtype=torch_long specifies that the elements of the tensor are long integers, suitable for holding indices.\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.long)\n",
    "\n",
    "    # Enumerate over the list of sequences to get both the index (i) and the sequence itself (seq) for each sequence in\n",
    "    # the list.\n",
    "    for i, seq in enumerate(sequences):\n",
    "        # Determine the length to which the current sequence should be padded. This is the smaller of max_length or the\n",
    "        # sequence's actual length.\n",
    "        # This ensures that sequences longer than max_length are truncated to max_length.\n",
    "        length = min(max_length, len(seq))\n",
    "\n",
    "        # Update the ith row of padded_sequences to include the first 'length' elements of the current sequence.\n",
    "        # torch_tensor(seq[:length], dtype=torch_long) creates a tensor from the first 'length' elements of seq,\n",
    "        # ensuring that the data type is still long integer. The tensor is then assigned to the first 'length' positions\n",
    "        # in the ith row of padded_sequences, effectively padding the sequence with zeros if it is shorter than\n",
    "        # max_length.\n",
    "        padded_sequences[i, :length] = torch.tensor(seq[:length], dtype=torch.long)\n",
    "\n",
    "    # After processing all sequences, return the tensor containing all the padded sequences.\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aef9593d-b67d-4379-8f6e-b9f8f1499b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class named SarcasmModel, which inherits from nn.Module. This class represents a neural network model.\n",
    "class SarcasmModel(nn.Module):\n",
    "    # The constructor method initializes the neural network with layers specified by the given parameters.\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, max_length):\n",
    "        # Initialize the base class (nn.Module).\n",
    "        super(SarcasmModel, self).__init__()\n",
    "\n",
    "        # Embedding layer that converts token indices to dense vectors of a specified size (embedding_dim).\n",
    "        # The 'padding_idx=0' argument ensures that the padding token (index 0) maps to a zero vector.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        # Dropout layer for regularization, randomly zeroes some of the elements of the input tensor with probability\n",
    "        # 0.2.\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # 1D Convolutional layer that applies 32 filters of size 5 to the input embeddings.\n",
    "        self.conv1d = nn.Conv1d(embedding_dim, 32, kernel_size=5)\n",
    "\n",
    "        # Max pooling layer that applies a 1D max pooling over an input signal composed of several input planes, using a\n",
    "        # window of size 4.\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=4)\n",
    "\n",
    "        # LSTM layer for processing sequences, with 32 input features and 64 hidden units, bidirectional processing.\n",
    "        self.lstm = nn.LSTM(32, 64, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Adaptive average pooling layer to convert the output of LSTM to a fixed size output, facilitating connection\n",
    "        # to dense layers.\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Fully connected layer that maps the LSTM output to a vector of size 128.\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "\n",
    "        # Second fully connected layer that maps the 128-sized vector to the final output size.\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    # The forward method defines the forward pass of the input through the model.\n",
    "    def forward(self, x):\n",
    "        # Pass the input x through the embedding layer.\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Permute the dimensions of x to match the expected input shape of Conv1D (batch_size, channels,\n",
    "        # sequence_length).\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Apply dropout for regularization.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply the ReLU activation function to the output of the convolutional layer.\n",
    "        x = F.relu(self.conv1d(x))\n",
    "\n",
    "        # Apply max pooling to reduce the dimensionality and to extract the most significant features.\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Permute the dimensions back to match the expected input shape of LSTM (batch_size, sequence_length, channels).\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Pass the result through the LSTM layer. Only the output tensor is used, and the hidden state is ignored.\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        # Permute again for the adaptive average pooling layer.\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Apply adaptive average pooling and remove the unnecessary extra dimension.\n",
    "        x = self.global_avg_pool(x).squeeze(2)\n",
    "\n",
    "        # Apply ReLU activation function to the output of the first fully connected layer.\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Apply dropout again.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply the sigmoid activation function to the output of the second fully connected layer to get the final model\n",
    "        # output.\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        # Return the final output.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8f08cef-24dc-4029-85ed-479d43d842bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train_model function with parameters for the model, data loaders for training and validation,\n",
    "# the device to run the training on (CPU or GPU), and the number of epochs to train for.\n",
    "def train_model(model, train_loader, val_loader, device, epochs=10):\n",
    "    # Define the loss function to be Binary Cross-Entropy, suitable for binary classification tasks.\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Initialize the optimizer to use the Adam algorithm for optimizing the model parameters.\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Transfer the model to the specified device (CPU or GPU).\n",
    "    model.to(device)\n",
    "\n",
    "    # Loop over the dataset multiple times, each loop is an epoch.\n",
    "    for epoch in range(epochs):\n",
    "        # Set the model to training mode. This is necessary because certain layers like Dropout behave differently\n",
    "        # during training.\n",
    "        model.train()\n",
    "\n",
    "        # Initialize a variable to keep track of the accumulated loss over the epoch.\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over the training data (inputs and labels).\n",
    "        for inputs, labels in train_loader:\n",
    "            # Transfer the inputs and labels to the specified device.\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients to prevent accumulation from previous iterations.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: compute the predicted outputs by passing inputs to the model and squeeze the output if\n",
    "            # necessary.\n",
    "            outputs = model(inputs).squeeze(1)\n",
    "\n",
    "            # Compute the loss between the predicted outputs and the actual labels.\n",
    "            loss = criterion(outputs, labels.float())\n",
    "\n",
    "            # Backward pass: compute the gradient of the loss with respect to the model parameters.\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform a single optimization step (parameter update).\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss over the batch to later calculate the average loss for the epoch.\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print the average loss for the epoch.\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "        # Switch the model to evaluation mode. This is necessary because certain layers like Dropout behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize counters for the total number of labels and the number of correct predictions.\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        # Disable gradient computation since it's not needed for validation, which saves memory and computations.\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation data.\n",
    "            for inputs, labels in val_loader:\n",
    "                # Transfer the inputs and labels to the specified device.\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass: compute the predicted outputs by passing inputs to the model and squeeze the output if\n",
    "                # necessary.\n",
    "                outputs = model(inputs).squeeze(1)\n",
    "\n",
    "                # Apply a threshold of 0.5 to the outputs to obtain the binary predictions.\n",
    "                predicted = outputs.round()\n",
    "\n",
    "                # Update the total count of labels processed.\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Update the count of correct predictions.\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print the accuracy for the current epoch, calculated as the percentage of correct predictions.\n",
    "        print(f'Accuracy after epoch {epoch + 1}: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cecaa3e-7c9e-4269-9e99-9f0e57e62821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's accuracy on a test dataset. This function calculates the percentage of correct predictions made by the model when\n",
    "# compared to the actual labels in the test dataset. It sets the model to evaluation mode to ensure that operations such as dropout are\n",
    "# not applied during the inference. The accuracy computation is performed in a no-gradient context to optimize memory usage and \n",
    "# computational efficiency, which is particularly important for large models or datasets. By iterating over batches of test data, the \n",
    "# function aggregates the total count of correct predictions and the total number of samples processed. These counts are used to calculate\n",
    "# the overall accuracy of the model on the test dataset. This method provides a straightforward and efficient means of assessing model \n",
    "# performance, which is crucial for the iterative process of model development and refinement.\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    # Ensure the model is in evaluation mode. This is necessary because certain modules like Dropout\n",
    "    # behave differently during training vs evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables to keep track of the total number of correct predictions and the total number\n",
    "    # of examples processed.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Disable gradient calculations to save memory and computations since they are not needed for evaluation.\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            # Unpack the batch of test data. Each batch consists of inputs (sentences) and their corresponding labels.\n",
    "            inputs, labels = data\n",
    "            # Transfer the data to the specified device.\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass: compute the predicted outputs by passing the inputs through the model.\n",
    "            outputs = model(inputs).squeeze(1)\n",
    "            \n",
    "            # Apply a threshold of 0.5 to the outputs to classify them as 0 or 1.\n",
    "            predicted = outputs.round()\n",
    "\n",
    "            # Update the total number of examples processed.\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Update the total number of correct predictions.\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate the accuracy as the percentage of correct predictions.\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b134760-dcd7-4e9f-8b08-daf943b73861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Function to visualize test results or model predictions\n",
    "def visualize_predictions(model, device, test_loader, num_images=5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Randomly sample 'num_images' indices from the test dataset\n",
    "    indices = random.sample(range(len(test_loader.dataset)), num_images)\n",
    "\n",
    "    # Initialize lists to store sampled images and labels\n",
    "    sampled_images = []\n",
    "    sampled_labels = []\n",
    "\n",
    "    # Obtain the images and labels for the randomly sampled indices\n",
    "    for idx in indices:\n",
    "        image, label = test_loader.dataset[idx]\n",
    "        sampled_images.append(image)\n",
    "        sampled_labels.append(label)\n",
    "\n",
    "    # Stack the list of images into a batch and transfer to the device\n",
    "    image_batch = torch.stack(sampled_images).to(device)\n",
    "\n",
    "    # Generate predictions from the model by passing the batch of images through it\n",
    "    output = model(image_batch)\n",
    "    _, preds = torch.max(output, 1)\n",
    "\n",
    "    # Create a new figure with specified width and height\n",
    "    plt.figure(figsize=(15, 3))\n",
    "\n",
    "    # Display each sampled image with its prediction and actual label\n",
    "    for idx in range(num_images):\n",
    "        ax = plt.subplot(1, num_images, idx + 1)\n",
    "        img = sampled_images[idx].numpy().transpose(1, 2, 0)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'Predicted: {preds[idx].item()}\\nActual: {sampled_labels[idx]}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7e14836-8e2f-446b-8a20-7c0273726570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sarcasm(sentences, model, vocab, tokenizer, max_length):\n",
    "    # Encode and pad the sentences\n",
    "    encoded_sentences = encode_sentences(sentences, vocab, tokenizer)\n",
    "    padded_sentences = pad_sequences(encoded_sentences, max_length)\n",
    "    \n",
    "    # Determine the device your model is on (either 'cuda' or 'cpu')\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Convert to tensor and move to the same device as the model\n",
    "    sentence_tensor = torch.tensor(padded_sentences, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        predictions = model(sentence_tensor).squeeze(1)\n",
    "        predicted_labels = predictions.round().int().tolist()  # Convert to binary labels\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c74600ee-f310-412d-a3ba-6a2f28eeae9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5190266884585961\n",
      "Accuracy after epoch 1: 79.408461250468%\n",
      "Epoch 2, Loss: 0.41035524395076517\n",
      "Accuracy after epoch 2: 81.87944590041182%\n",
      "Epoch 3, Loss: 0.35979067195365405\n",
      "Accuracy after epoch 3: 82.77798577311869%\n",
      "Epoch 4, Loss: 0.32734862722932023\n",
      "Accuracy after epoch 4: 83.26469487083489%\n",
      "Epoch 5, Loss: 0.3004511218280234\n",
      "Accuracy after epoch 5: 83.00262074129539%\n",
      "Epoch 6, Loss: 0.2807916783866413\n",
      "Accuracy after epoch 6: 83.97603893672782%\n",
      "Epoch 7, Loss: 0.26474408892310286\n",
      "Accuracy after epoch 7: 83.82628229127667%\n",
      "Epoch 8, Loss: 0.24732636479769854\n",
      "Accuracy after epoch 8: 83.52676900037439%\n",
      "Epoch 9, Loss: 0.2360179844173662\n",
      "Accuracy after epoch 9: 84.0134780980906%\n",
      "Epoch 10, Loss: 0.22186123802663482\n",
      "Accuracy after epoch 10: 83.67652564582554%\n"
     ]
    }
   ],
   "source": [
    "# URL pointing to the sarcasm dataset.\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
    "\n",
    "# Download the dataset and save it locally as 'sarcasm.json'.\n",
    "urlretrieve(url, 'sarcasm.json')\n",
    "\n",
    "# Initialize lists to hold sentences and their corresponding sarcasm labels.\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "# Open the downloaded JSON file for reading.\n",
    "with open('sarcasm.json', 'r') as file:\n",
    "    # Load the JSON content.\n",
    "    data = json_load(file)\n",
    "\n",
    "    # Iterate over each record in the dataset, extracting the sentence and its sarcasm label.\n",
    "    for item in data:\n",
    "        sentences.append(item['headline'])  # Add the sentence to the list.\n",
    "        labels.append(item['is_sarcastic'])  # Add the sarcasm label (0 or 1) to the list.\n",
    "\n",
    "# Set key parameters for the model and data processing.\n",
    "vocab_size = 1000\n",
    "max_length = 120\n",
    "embedding_dim = 100\n",
    "output_dim = 1\n",
    "\n",
    "# Obtain a tokenizer for basic English to convert sentences into tokens.\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Create a vocabulary from the list of sentences with the specified maximum size.\n",
    "vocab = create_vocab(sentences, vocab_size)\n",
    "\n",
    "# Encode the sentences into sequences of indices based on the vocabulary.\n",
    "encoded_sentences = encode_sentences(sentences, vocab, tokenizer)\n",
    "\n",
    "# Pad the encoded sequences to ensure they all have the same length.\n",
    "padded_sentences = pad_sequences(encoded_sentences, max_length)\n",
    "\n",
    "# Convert the labels list into a tensor of type float32.\n",
    "labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "# Split the padded sentences and labels into training and testing sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_sentences, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create dataset objects for training and testing data.\n",
    "train_dataset = SarcasmDataset(x_train, y_train)\n",
    "test_data = SarcasmDataset(x_test, y_test)\n",
    "\n",
    "# Initialize DataLoader objects for batching and shuffling the training and testing datasets.\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the sarcasm detection model with the specified parameters.\n",
    "model = SarcasmModel(len(vocab), embedding_dim, output_dim, max_length)\n",
    "\n",
    "# Train the model using the training and validation DataLoader objects, on the specified device.\n",
    "train_model(model, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a00d2c61-98f7-4ee3-9db9-1b088088e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually define class names for the sarcasm detection task\n",
    "class_names = ['Not Sarcastic', 'Sarcastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4106289d-be88-4a94-a8a9-99a2d3a7f758",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m quantized_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../models/quantized_exercise_4_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Extract the directory paths\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(model_path)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create the directories if they do not exist\u001b[39;00m\n\u001b[1;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(model_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the model paths\n",
    "model_path = \"../../models/exercise_4_model.pth\"\n",
    "quantized_model_path = \"../../models/quantized_exercise_4_model.pth\"\n",
    "\n",
    "# Extract the directory paths\n",
    "model_dir = os.path.dirname(model_path)\n",
    "\n",
    "# Create the directories if they do not exist\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcfe0b5d-4d6b-4a31-b466-74d892da37f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.68%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "test_accuracy = evaluate_model(model, test_loader, device)\n",
    "print(f'Test Accuracy: {round(test_accuracy, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4dc8f0-4c6a-4423-bfae-8a6234f4de55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cebb13c-311f-4fb8-a212-08a034ca9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the final trained model's state dictionary to a file named '../models/exercise_1_model.pth'. This file can\n",
    "# be used later for further use or deployment.\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2de23fea-eb6d-4188-8c2a-23fea3c25d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 735020 bytes, or 0.70 MB\n"
     ]
    }
   ],
   "source": [
    "# Get the size of the model\n",
    "model_size = os.path.getsize(model_path)\n",
    "\n",
    "# Convert size to more readable format (e.g., in MB)\n",
    "model_size_mb = model_size / (1024 * 1024)\n",
    "\n",
    "print(f\"Model size: {model_size} bytes, or {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d819180-ed2d-40ec-af14-4c572683773a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: irish weather forecaster's halloween report spooks viewers, Actual: Not Sarcastic, Predicted Sarcasm: Not Sarcastic\n",
      "Sentence: this u.s. district could 'demolish the glass ceiling' in november with first all-female ticket, Actual: Not Sarcastic, Predicted Sarcasm: Not Sarcastic\n",
      "Sentence: meet the man who helps hollywood stay sober, Actual: Not Sarcastic, Predicted Sarcasm: Not Sarcastic\n",
      "Sentence: man proposes to girlfriend on romantic plane ride, immediately throws up, Actual: Not Sarcastic, Predicted Sarcasm: Sarcastic\n",
      "Sentence: 12 ways to make your divorce as expensive as possible, Actual: Not Sarcastic, Predicted Sarcasm: Not Sarcastic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9422/1391151877.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentence_tensor = torch.tensor(padded_sentences, dtype=torch.long).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Assuming `sentences` and `labels` are your lists of sentences and labels respectively\n",
    "random_indices = random.sample(range(len(sentences)), 5)  # Select 5 random indices\n",
    "random_sentences = [sentences[i] for i in random_indices]  # Get the corresponding sentences\n",
    "actual_labels = [labels[i] for i in random_indices]  # Get the corresponding actual labels\n",
    "\n",
    "# Predict sarcasm status for these sentences\n",
    "predicted_labels = predict_sarcasm(random_sentences, model, vocab, tokenizer, max_length)\n",
    "for sentence, actual, predicted in zip(random_sentences, actual_labels, predicted_labels):\n",
    "    print(f\"Sentence: {sentence}, Actual: {'Sarcastic' if actual == 1 else 'Not Sarcastic'}, Predicted Sarcasm: {'Sarcastic' if predicted == 1 else 'Not Sarcastic'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df9f5133-079a-492c-b39d-20373a48c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'model' is your pre-trained FashionMNIST model\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "# Move your model to CPU (quantization is typically done for CPU inference)\n",
    "model.to('cpu')\n",
    "\n",
    "# Apply dynamic quantization\n",
    "# Note: Changing dtype to torch.qint8 for quantization, which is a common practice\n",
    "quantized_model = quantize_dynamic(\n",
    "    model, \n",
    "    {torch.nn.Linear},  # Specify the layer types to quantize\n",
    "    dtype=torch.qint8  # Use qint8 for quantizing the weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46213455-4bde-4b75-b7a1-807aef2c2af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.68%\n"
     ]
    }
   ],
   "source": [
    "# Evaulate Accuracy\n",
    "test_accuracy = evaluate_model(quantized_model, test_loader, 'cpu')\n",
    "print(f'Test Accuracy: {round(test_accuracy, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8623e89e-e463-461e-be78-5476fac2d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the final trained model's state dictionary. This file can be used later for further use or deployment.\n",
    "torch.save(quantized_model.state_dict(), quantized_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15fd9441-58ff-4b97-bed3-04ce5221f967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model size: 688466 bytes, or 0.66 MB\n"
     ]
    }
   ],
   "source": [
    "# Get the size of the model\n",
    "model_size = os.path.getsize(quantized_model_path)\n",
    "\n",
    "# Convert size to more readable format (e.g., in MB)\n",
    "model_size_mb = model_size / (1024 * 1024)\n",
    "\n",
    "print(f\"Quantized Model size: {model_size} bytes, or {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ef7e4a9-77b1-4d6a-8bf6-5784f566846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: trump confident u.s. military strike on syria wiped out russian scandal, Actual: Sarcastic, Predicted Sarcasm: Not Sarcastic\n",
      "Sentence: man allegedly kidnaps girl he met on 'disney fairies' website, Actual: Not Sarcastic, Predicted Sarcasm: Sarcastic\n",
      "Sentence: 'rolling stone' offering readers 3-month free trial period for buying company, Actual: Sarcastic, Predicted Sarcasm: Sarcastic\n",
      "Sentence: hillary clinton hints at presidential ambitions by concealing information from american people, Actual: Sarcastic, Predicted Sarcasm: Sarcastic\n",
      "Sentence: evangelical christians enter 10th day of vigil outside your house, Actual: Sarcastic, Predicted Sarcasm: Sarcastic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9422/1391151877.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentence_tensor = torch.tensor(padded_sentences, dtype=torch.long).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Assuming `sentences` and `labels` are your lists of sentences and labels respectively\n",
    "random_indices = random.sample(range(len(sentences)), 5)  # Select 5 random indices\n",
    "random_sentences = [sentences[i] for i in random_indices]  # Get the corresponding sentences\n",
    "actual_labels = [labels[i] for i in random_indices]  # Get the corresponding actual labels\n",
    "\n",
    "# Predict sarcasm status for these sentences\n",
    "predicted_labels = predict_sarcasm(random_sentences, quantized_model, vocab, tokenizer, max_length)\n",
    "for sentence, actual, predicted in zip(random_sentences, actual_labels, predicted_labels):\n",
    "    print(f\"Sentence: {sentence}, Actual: {'Sarcastic' if actual == 1 else 'Not Sarcastic'}, Predicted Sarcasm: {'Sarcastic' if predicted == 1 else 'Not Sarcastic'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
